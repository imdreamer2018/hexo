title: Deep Learning——Deep Convolutional Neural Networks

author: 追梦人

toc: true

categories: []

date: 2018-3-14 19:12:00

tags:

 - DeepLearning

 - Convolution
---

# 深度卷积神经网络

此教材来自吴恩达（Andrew Ng）<a href=""http://mooc.study.163.com/course/2001281004#/info>网易公开课</a>
<!--more-->

## 一.残差网络(ResNets)

我们知道非常非常深的网络是很难训练的，因为存在**梯度消失**和**梯度爆炸**问题，所以我们可以利用跳远连接构建能够训练深度网络的**ResNets**，有时深度能到超过100层。

###1.残差块(Residual block)

ResNets是由残差块构建的，**残差块**：这是一个两层神经网络 在L层进行激活,得到a[l+1]，再次进行激活 ，两层之后得到a[l+2]，计算过程是 从a[l]开始，首先进行线性激活，根据这个等式 通过a[l]算出z[l+1] ，即a[l]乘以权重矩阵 再加上偏差因子，然后通过ReLU非线性激活得到a[l+1] ，由a[l+1]等于g(z[l+1])计算得出，接着再次进行线性激活，依据的等式是这个 𝑧[𝑙+2] = 𝑊[𝑙+2]𝑎[𝑙+1] +𝑏[𝑙+2] ，最后根据这个等式再次进行ReLU非线性激活，得到的结果就是a[l+2]。换句话说 信息流从a[l]到a[l+2]，需要经过以上所有步骤，即这组网络层的主路径，在残差网络中有一点变化 ，我们将a[l]直接向后，拷贝到神经网络的深层，在ReLU非线性激活前加上a[l]，这是一条捷径（除了捷径 你还会听到另一个术语**跳远连接**），a[l]的信息直接到达神经网络的深层，不再沿着主路径传递，这就意味着最后这个等式𝑎[𝑙+2] = 𝑔(𝑧[𝑙+2])去掉了，取而代之的是另一个ReLU非线性函数𝑎[𝑙+2] = 𝑔(𝑧[𝑙+2]+ 𝑎[𝑙] )，也就是加上的这个a[l]产生了一个**残差块**。**残差网络**(ResNet)的发明者是何恺明，张翔宇，任少卿和孙剑，他们发现使用残差块能够训练更深的神经网络，所以构建一个ResNet网络就是通过将很多这样的残差块，堆积在一起 形成一个深度神经网络。

![Residual block](http://imgss.lovebingzi.com/Deep-Convolution-Neural-Networks/Residual%20block.png)

##2.残差网络(Residual Network)

这并不是一个残差网络 而是一个普通网络，这个术语来自ResNet的论文 ，把它变成ResNet的方法是加上所有的跳远连接，或者说捷径，像这样，每两层增加一个捷径，构成一个残差块，如图所示 5个残差块连接在一起，构成一个残差网络。如果我们使用标准优化算法训练一个普通网络，比如说梯度下降，或者其他热门的优化算法，如果没有多余的残差 没有这些捷径，或者跳远连接，凭经验，**你会发现随着网络深度的加深，训练错误会先减少，然后增多，而理论上 随着网络深度的加深，应该训练得越来越好才对 ，也就是说 理论上网络深度越深越好，但实际上 如果没有残差网络，对于一个普通网络来说 深度越深意味着，用优化算法越难训练，随着网络深度的加深 训练错误会越来越多**。**但是有了ResNets就不一样了 即使网络再深，练的表现却不错 比如说出错误会减少，就算是训练深达100层的网络也不例外。但是对x的激活 或者这些中间的激活，能够达到网络的更深层，这种方式确实有助于解决梯度消失和梯度爆炸问题，让我们在训练更深网络的同时，又能保证良好的性能，ResNet确实在训练深度网络方面非常有效。**

至于为什么ResNets能有如此好的表现，请看下节。

![Residual Network](http://imgss.lovebingzi.com/Deep-Convolution-Neural-Networks/Residual%20Network.png)

### 3.残差网络为什么有如此好的表现？

我们来看一个例子 它解释了其中的原因，至少可以说明 如何在构建更深层次的ResNet网络的同时，还不降低它们在训练集上的效率，通常来讲 网络在训练集上表现好，才能在Hold-Out交叉验证集，或dev集和测试集上有好的表现，至少在训练集上训练好ResNet是第一步。

上节我们知道，一个网络深度越深 它在训练集上训练网络的效率会有所减弱，这也是有时候我们不希望加深网络的原因，而事实并非如此，至少在训练ResNet网络时 并不完全如此，举个例子，假设有一个大型神经网络 其输入为X，输出激活值a[l]，如果你想增加这个神经网络的深度，那么用Big NN表示 输出为a[l]，再给这个网络额外添加两层，依次添加两层，最后输出为a[l+2]，可以把这两层看作一个ResNet块，即具有近路连接的残差块，为了方便说明，假设我们在整个网络中使用ReLu激活函数，所有激活值都大于等于0，包括输入X的非零异常值，因为ReLu激活函数输出的数字要么是0 要么是正数，我们看一下a[l+2]的值，也就是表达式即a[l+2] =g(z[l+2]+a[l])，添加项a[l]是刚添加的跳远连接的输入，展开这个表达式 a[l+2]=g(W[l+2])a([l+1]+b[l+2]+a[l])，注意一点 如果使用L2正则化或权重衰减，它会压缩W[l+2]的值，如果对b应用权重衰减 亦可达到同样的效果，尽管实际应用中 你有时会对b应用权重衰减 有时不会，这里的W是关键项，如果W[l+2]=0，为方便起见 假设b也等于0，这几项就没有了 因为它们的值为0，最后等于g(a[l]) 也就是a[l]，因为我们假定使用ReLu激活函数，并且所有激活值都是负的，g(a[l]) 是应用于非负数的ReLu函数，所以a[l+2]等于a[l]，结果表明 **残差块学习这个恒等式函数残差块并不难，跳远连接使我们很容易得出a[l+2]=a[1]，这意味着 即使给神经网络增加了这两层，它的效率也并不逊色于更简单的神经网络 ，因为学习恒等函数对它来说很简单，尽管它多了两层 也只是把a[l]的值赋给a[l+2]，所以 给大型神经网络增加两层，不论是把残差块添加到神经网络的中间还是末端位置，都不会影响网络的表现，当然 我们的目标不仅仅是保持网络效率，还要提升它的效率，想象一下 如果这些隐层单元学到一些有用信息，那么它可能比学习恒等函数表现得更好，而这些不含有残差块或跳远连接的深度普通网络，情况就不一样了，当网络不断加深时，就算是选择用来学习恒等函数的参数都很困难，所以很多层最后的表现不但没有更好 反而更糟，我认为 残差网络起作用的主要原因就是，这些残差块学习恒等函数非常容易，你能确定网络性能不会受到影响 ，很多时候甚至可以提高效率，或者说至少不会降低网络效率，因此创建类似残差网络可以提升网络性能。**除此之外 关于残差网络 另一个值得探讨的细节是，假设z[l+2]与a[l]具有相同维度，所以ResNets使用了许多相同卷积过滤算法，所以这个a[l]的维度等于这个输出层的维度，因而实现了这个跳远连接，因为同一个卷积保留了维度，所以很容易得出这个短连接，并输出这两个相同维度的向量。

![why](http://imgss.lovebingzi.com/Deep-Convolution-Neural-Networks/why.png)